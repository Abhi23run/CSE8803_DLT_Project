{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5Model, T5ForConditionalGeneration, T5TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../finqa_dataset/train.json\") as input_file:\n",
    "        train_data = json.load(input_file)\n",
    "\n",
    "with open(\"../finqa_dataset/dev.json\") as input_file:\n",
    "        valid_data = json.load(input_file)\n",
    "\n",
    "with open(\"../finqa_dataset/test.json\") as input_file:\n",
    "        test_data = json.load(input_file)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6251 883 1147\n"
     ]
    }
   ],
   "source": [
    "print (len(train_data),len(valid_data),len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ad_data(json_data, verbose=True):\n",
    "    net=[]\n",
    "    err_cnt=0\n",
    "    for example in json_data:\n",
    "        question = example[\"qa\"][\"question\"]\n",
    "        table = example[\"table\"]\n",
    "        table_text = \"\"\n",
    "        for row in table[1:]:\n",
    "            this_sent = table_row_to_text(table[0], row)\n",
    "            table_text += this_sent\n",
    "        try:\n",
    "            steps_text = format_steps(example[\"qa\"][\"steps\"])\n",
    "            inputs = {\"context\": table_text, \"question\": question, \"answer\": steps_text} \n",
    "            net.append(inputs)\n",
    "        except:\n",
    "            err_cnt+=1\n",
    "            if verbose:\n",
    "                print (\"-\"*25)\n",
    "                print (example[\"filename\"])\n",
    "                print (example[\"qa\"][\"steps\"])\n",
    "                print (\"-\"*25+\"\\n\")\n",
    "    if err_cnt>0:\n",
    "        print (\"Net Errors:\",err_cnt)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net Errors: 198\n",
      "Net Errors: 35\n",
      "Net Errors: 39\n"
     ]
    }
   ],
   "source": [
    "train_pd = pd.DataFrame(ad_data(train_data,False))\n",
    "valid_pd = pd.DataFrame(ad_data(valid_data,False))\n",
    "test_pd = pd.DataFrame(ad_data(test_data,False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6053, 3), (848, 3), (1108, 3))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pd.shape, valid_pd.shape, test_pd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "# MODEL = T5ForConditionalGeneration.from_pretrained(\"t5-base\", return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 32100\n"
     ]
    }
   ],
   "source": [
    "vocab_size = TOKENIZER.vocab_size\n",
    "print(\"Vocabulary Size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinQA_Dataset(Dataset):\n",
    "    def __init__(self, tokenizer, df, max_q_len, max_a_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.q_len = max_q_len\n",
    "        self.t_len = max_a_len\n",
    "        self.data = df\n",
    "        self.questions = self.data[\"question\"]\n",
    "        self.context = self.data[\"context\"]\n",
    "        self.answer = self.data['answer']        \n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "    def __getitem__(self, idx):\n",
    "        question = self.questions[idx]\n",
    "        context = self.context[idx]\n",
    "        answer = self.answer[idx]\n",
    "        question_tokenized = self.tokenizer(question, context, max_length=self.q_len, padding=\"max_length\",\n",
    "                                                    truncation=True, pad_to_max_length=True, add_special_tokens=True)\n",
    "        answer_tokenized = self.tokenizer(answer, max_length=self.t_len, padding=\"max_length\", \n",
    "                                          truncation=True, pad_to_max_length=True, add_special_tokens=True)\n",
    "        labels = torch.tensor(answer_tokenized[\"input_ids\"], dtype=torch.long)\n",
    "        labels[labels == 0] = -100\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(question_tokenized[\"input_ids\"], dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(question_tokenized[\"attention_mask\"], dtype=torch.long),\n",
    "            \"labels\": labels,\n",
    "            \"decoder_attention_mask\": torch.tensor(answer_tokenized[\"attention_mask\"], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_Q_LEN = 1024\n",
    "MAX_A_LEN = 128 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "finqa_train = FinQA_Dataset(TOKENIZER, train_pd, MAX_Q_LEN, MAX_A_LEN)\n",
    "finqa_valid = FinQA_Dataset(TOKENIZER, valid_pd, MAX_Q_LEN, MAX_A_LEN)\n",
    "finqa_test = FinQA_Dataset(TOKENIZER, test_pd, MAX_Q_LEN, MAX_A_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "context     the fair value of forward exchange contracts a...\n",
       "question            what is the the interest expense in 2009?\n",
       "answer      Step 1: Divide 100 by 100. This gives the resu...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pd.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([125,  19,   8,  ...,   0,   0,   0]),\n",
       " 'attention_mask': tensor([1, 1, 1,  ..., 0, 0, 0]),\n",
       " 'labels': tensor([ 5021,   209,    10, 22390,   910,    57,   910,     5,   100,  1527,\n",
       "             8,   741,    10,     3,  4704,  1713, 30345, 30345, 30345,  5021,\n",
       "           204,    10, 22390,     3, 26195,    57,     3,  4704,     5,   100,\n",
       "          1527,     8,   741,    10,     3, 22671,     1,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]),\n",
       " 'decoder_attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finqa_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5021,   209,    10, 22390,   910,    57,   910,     5,   100,  1527,\n",
       "            8,   741,    10,     3,  4704,  1713, 30345, 30345, 30345,  5021,\n",
       "          204,    10, 22390,     3, 26195,    57,     3,  4704,     5,   100,\n",
       "         1527,     8,   741,    10,     3, 22671,     1,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finqa_train[0]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Step 1: Divide 100 by 100. This gives the result: 1% ####### Step 2: Divide 3.8 by 1%. This gives the result: 380'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pd.iloc[0][\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Step 1: Divide 100 by 100. This gives the result: 1% ####### Step 2: Divide 3.8 by 1%. This gives the result: 380'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOKENIZER.decode([x for x in finqa_train[0]['labels'] if x!= -100], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'op': 'divide1-1', 'arg1': '100', 'arg2': '100', 'res': '1%'},\n",
       " {'op': 'divide1-2', 'arg1': '3.8', 'arg2': '#0', 'res': '380'}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][\"qa\"][\"steps\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(finqa_train, '../finqa_dataset/finqa_train.pth')\n",
    "torch.save(finqa_valid, '../finqa_dataset/finqa_valid.pth')\n",
    "torch.save(finqa_test, '../finqa_dataset/finqa_test.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlt_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
