{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install SentencePiece"
      ],
      "metadata": {
        "id": "1hSc4q-H1wE3",
        "outputId": "eecc3583-ace3-4a49-87fe-bf1f185bba80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: SentencePiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HCsApGbJ0rs6"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import transformers\n",
        "from torch.optim import Adam\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: mount from google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "CIsQumys0zRU",
        "outputId": "b3e615f2-5f77-465a-dbf2-96e49e703f24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/MyDrive/dlt_results/"
      ],
      "metadata": {
        "id": "tOjlOh-c1DFK",
        "outputId": "2b3acc2d-1c31-4a78-9494-7eeefb245225",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/dlt_results\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hFVYFDqa0rs9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from transformers import T5Tokenizer, T5Model, T5ForConditionalGeneration, T5TokenizerFast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "hlljs3hX0rs9"
      },
      "outputs": [],
      "source": [
        "from utils import FinQA_Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "lecmvG760rs9",
        "outputId": "ff6892a8-7ad5-4fb1-cf52-56c2bdce1510",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/t5/tokenization_t5.py:240: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "TOKENIZER = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "MODEL = T5ForConditionalGeneration.from_pretrained(\"./finqa_finetune_t5.pth/\")\n",
        "BATCH_SIZE = 8\n",
        "TESTING = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "vnUzhYs50rs-",
        "outputId": "ed2f724f-7709-4a86-9cca-79c5bad268f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "DEVICE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL.to(DEVICE);"
      ],
      "metadata": {
        "id": "GQESZfYk2bbx"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_19x6Xuw0rs-"
      },
      "source": [
        "### Read Test and do inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "jX_CRufW0rtA",
        "outputId": "633e8c02-3d50-47a8-cb28-eb5383556174",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1108"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "finqa_test = torch.load('./finqa_test.pth')\n",
        "len(finqa_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "1aifITs70rtA"
      },
      "outputs": [],
      "source": [
        "if TESTING:\n",
        "    test_loader = DataLoader(Subset(finqa_test, range(100)), batch_size=BATCH_SIZE)\n",
        "else:\n",
        "    test_loader = DataLoader(finqa_test, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "0Wwc4WaI0rtA",
        "outputId": "653585fd-3e8e-4594-ab51-a4ab10054468",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Step 1: Subtract 5735 from 5829. This gives the result: 94'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "TOKENIZER.decode([x for x in finqa_test[0]['labels'] if x!= -100], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "CaXBX7XZ0rtB",
        "outputId": "2b13379f-7752-4ea5-ac82-a339dabdc878",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGeneration batches:   0%|          | 0/139 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Generation batches:   2%|▏         | 3/139 [00:08<06:36,  2.92s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Generation batches:   8%|▊         | 11/139 [00:33<06:40,  3.13s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Generation batches:   9%|▊         | 12/139 [00:36<06:41,  3.16s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Generation batches:  20%|██        | 28/139 [01:25<05:42,  3.09s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Generation batches:  32%|███▏      | 45/139 [02:21<05:21,  3.42s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Generation batches:  39%|███▉      | 54/139 [02:45<03:46,  2.66s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Generation batches:  53%|█████▎    | 74/139 [03:47<03:13,  2.98s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Generation batches:  63%|██████▎   | 88/139 [04:25<02:08,  2.52s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Generation batches:  66%|██████▌   | 92/139 [04:36<01:52,  2.38s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Generation batches:  67%|██████▋   | 93/139 [04:39<02:01,  2.65s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Generation batches:  78%|███████▊  | 108/139 [05:26<01:37,  3.15s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Generation batches:  80%|███████▉  | 111/139 [05:32<01:17,  2.78s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Generation batches:  81%|████████▏ | 113/139 [05:38<01:14,  2.88s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Generation batches:  88%|████████▊ | 123/139 [06:07<00:41,  2.60s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Generation batches:  98%|█████████▊| 136/139 [06:44<00:07,  2.47s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Generation batches:  99%|█████████▉| 138/139 [06:51<00:02,  2.88s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Generation batches: 100%|██████████| 139/139 [06:53<00:00,  2.97s/it]\n"
          ]
        }
      ],
      "source": [
        "MODEL.eval();\n",
        "generated_texts = []\n",
        "for batch in tqdm(test_loader, desc=\"Generation batches\"):\n",
        "    input_ids = batch[\"input_ids\"].to(DEVICE)\n",
        "    attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
        "    decoder_attention_mask = batch[\"decoder_attention_mask\"].to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        # Use the `generate` method for text generation\n",
        "        generated_output = MODEL.generate(\n",
        "            input_ids=input_ids,\n",
        "            max_length=128,  # Set the desired maximum length for generated text\n",
        "            num_beams=5,    # You can adjust the number of beams for beam search\n",
        "            temperature=0.8  # You can adjust the temperature for sampling,\n",
        "            )\n",
        "        for output_i in generated_output:\n",
        "          generated_text = TOKENIZER.decode([x for x in output_i if x!= -100], skip_special_tokens=True)\n",
        "          generated_texts.append(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "lcuJKRyA0rtB",
        "outputId": "3460bfb3-1252-4017-c63d-da8aeb51638f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1108"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "len(generated_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Db04JeE-0rtC"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "file_path = './generated_sentences_test.pkl'\n",
        "# Open the file in binary read mode and use pickle.load() to load the list\n",
        "with open(file_path, 'wb') as file:\n",
        "    pickle.dump(generated_texts, file)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}